from langchain_core.messages import HumanMessage
from langchain_anthropic import ChatAnthropic
from src.state import NovelState
import os
import json
import time

def critic_node(state: NovelState) -> NovelState:
    """
    The Critic Node - comprehensive quality check.
    Evaluates complete content with intelligent truncation.
    æ”¯æŒç•ªèŒ„å°è¯´é£æ ¼è¯„å®¡æ ‡å‡†
    """
    print("--- CRITIC NODE ---")

    draft = state.get("draft", "")
    world_bible = state.get("world_bible", {})
    current_beats = state.get("current_beats", "")

    # æ™ºèƒ½æˆªå–:ä¿ç•™å®Œæ•´æ®µè½,é¿å…åœ¨å¥å­ä¸­é—´æˆªæ–­
    content_to_check = draft
    max_check_chars = 5000  # å¢åŠ åˆ°5000å­—ç¬¦,è¶³å¤Ÿæ£€æŸ¥3700å­—çš„ç« èŠ‚

    if len(draft) > max_check_chars:
        # æˆªå–å‰5000å­—ç¬¦,ç„¶åæ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´æ®µè½
        truncated = draft[:max_check_chars]
        # å¯»æ‰¾æœ€åçš„æ®µè½ç»“æŸä½ç½®
        last_paragraph_end = max(
            truncated.rfind('\n\n'),
            truncated.rfind('ã€‚\n'),
            truncated.rfind('ã€‚"'),
            truncated.rfind('ã€‚ã€')
        )
        if last_paragraph_end > 2000:  # ç¡®ä¿è‡³å°‘æœ‰2000å­—ç¬¦
            content_to_check = truncated[:last_paragraph_end + 1]
        else:
            content_to_check = truncated

        chars_checked = len(content_to_check)
        total_chars = len(draft)
        coverage = (chars_checked / total_chars) * 100
        print(f"  ğŸ“ å†…å®¹è¾ƒé•¿,æ£€æŸ¥å‰ {chars_checked}/{total_chars} å­—ç¬¦ ({coverage:.0f}% è¦†ç›–,å®Œæ•´æ®µè½)")
    else:
        print(f"  ğŸ“ æ£€æŸ¥å®Œæ•´å†…å®¹ ({len(draft)} å­—ç¬¦)")

    # æå–è§’è‰²çŠ¶æ€å’Œä¼ç¬”ï¼ˆå®Œæ•´ç‰ˆåŠŸèƒ½ï¼‰
    character_states = extract_character_context(world_bible.get('characters', {}))
    plot_threads = world_bible.get('plot_threads', [])

    # æ£€æŸ¥æ˜¯å¦ä¸ºç•ªèŒ„å°è¯´é£æ ¼
    config = state.get('config', {})
    style = config.get('style', {})
    is_fanqie = style.get('is_fanqie_style', False)

    # æ„å»ºè¯„å®¡ promptï¼ˆå®Œæ•´ç‰ˆï¼šå¤šç»´åº¦è¯„å®¡ï¼‰
    prompt_parts = [
        "ä½ æ˜¯èµ„æ·±å°è¯´ç¼–è¾‘ï¼Œè¿›è¡Œå…¨é¢æ·±åº¦è¯„å®¡ã€‚",
        "",
        "ã€è§’è‰²è®¾å®šã€‘",
        json.dumps(world_bible.get('characters', {}), ensure_ascii=False)[:600],
        "",
    ]

    # æ·»åŠ è§’è‰²çŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
    if character_states:
        prompt_parts.extend([
            "ã€è§’è‰²å½“å‰çŠ¶æ€ã€‘",
            character_states,
            ""
        ])

    # æ·»åŠ ä¼ç¬”çº¿ç´¢ï¼ˆå¦‚æœæœ‰ï¼‰
    if plot_threads:
        # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„ï¼ˆçŸ­ç¯‡: list, é•¿ç¯‡: dict with "active"ï¼‰
        if isinstance(plot_threads, dict):
            # é•¿ç¯‡æ¨¡å¼ï¼šä» dict ä¸­æå– active threads
            active_threads = plot_threads.get("active", [])[-5:]
        else:
            # çŸ­ç¯‡æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ list
            active_threads = plot_threads[-5:]

        # ğŸ”§ Bug #9ä¿®å¤: å¤„ç†dictæ ¼å¼çš„threadå¯¹è±¡
        formatted_threads = []
        for thread in active_threads:
            if isinstance(thread, dict):
                # é•¿ç¯‡æ¨¡å¼: threadæ˜¯ {"text": "...", ...}
                formatted_threads.append(thread.get("text", str(thread)))
            else:
                # çŸ­ç¯‡æ¨¡å¼: threadæ˜¯å­—ç¬¦ä¸²
                formatted_threads.append(str(thread))

        threads_text = "\n".join([f"- {t}" for t in formatted_threads])

        if threads_text:  # åªæœ‰åœ¨æœ‰å†…å®¹æ—¶æ‰æ·»åŠ 
            prompt_parts.extend([
                "ã€ç°æœ‰ä¼ç¬”/è°œå›¢ã€‘",
                threads_text,
                ""
            ])

    prompt_parts.extend([
        "ã€åœºæ™¯å¤§çº²ã€‘",
        current_beats[:400] if current_beats else "(æ— )",
        "",
        "ã€ç”Ÿæˆå†…å®¹ã€‘",
        content_to_check,
        "",
    ])

    # æ ¹æ®é£æ ¼é€‰æ‹©è¯„å®¡æ ‡å‡†
    if is_fanqie:
        print("  ğŸ¯ ä½¿ç”¨ç•ªèŒ„å°è¯´è¯„å®¡æ ‡å‡†")
        prompt_parts.extend([
            "ã€è¯„å®¡ç»´åº¦ - ç•ªèŒ„å°è¯´æ ‡å‡†ã€‘",
            "1. **èŠ‚å¥é€Ÿåº¦**: æ˜¯å¦å¼€é—¨è§å±±ï¼Œç›´æ¥è¿›å…¥å†²çªï¼Œæ— å†—é•¿é“ºå«",
            "2. **çˆ½ç‚¹å¯†åº¦**: æ¯ç« è‡³å°‘2-3ä¸ªçˆ½ç‚¹ï¼ˆæ‰“è„¸/åè½¬/æ”¶è·/ç¢¾å‹ï¼‰",
            "3. **å¯¹æ¯”å¼ºçƒˆ**: æ˜¯å¦æœ‰åˆ«äººvsä¸»è§’çš„å¼ºçƒˆå¯¹æ¯”",
            "4. **ç®€æ´ç›´ç™½**: é¿å…è¿‡åº¦å¿ƒç†æå†™ï¼Œå¤šç”¨åŠ¨ä½œå’Œå¯¹è¯",
            "5. **ä¸»è§’å¼ºåŠ¿**: ä¸»è§’æ˜¯å¦æ™ºå•†åœ¨çº¿ã€è¡ŒåŠ¨æœæ–­ã€å æ®ä¸»åŠ¨",
            "",
            "ã€ç•ªèŒ„é£æ ¼è¦æ±‚ã€‘",
            "- å†²çªâ†’çˆ½ç‚¹â†’å†²çªâ†’çˆ½ç‚¹ï¼Œå¿«èŠ‚å¥æ¨è¿›",
            "- é¿å…çº¯é“ºå«åœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯éƒ½è¦æœ‰å†²çªæˆ–æ”¶è·",
            "- æ‰“è„¸è¦ç‹ ï¼Œåè½¬è¦å¿«ï¼Œä¸æ‹–æ³¥å¸¦æ°´",
            "",
            "ã€å›ç­”æ ¼å¼ã€‘",
            "- å¦‚æœé€šè¿‡: 'é€šè¿‡ - çˆ½ç‚¹: [å…·ä½“çˆ½ç‚¹]'",
            "- å¦‚æœéœ€ä¿®æ”¹: 'éœ€ä¿®æ”¹ - é—®é¢˜: [èŠ‚å¥/çˆ½ç‚¹/å¯¹æ¯”ç­‰å…·ä½“é—®é¢˜]'",
            "",
            "è¯·æŒ‰ç•ªèŒ„å°è¯´æ ‡å‡†è¯„å®¡ã€‚"
        ])
    else:
        print("  ğŸ“– ä½¿ç”¨ä¼ ç»Ÿæ–‡å­¦è¯„å®¡æ ‡å‡†")
        prompt_parts.extend([
            "ã€è¯„å®¡ç»´åº¦ã€‘",
            "1. **è§’è‰²ä¸€è‡´æ€§**: è¡Œä¸º/å¯¹è¯æ˜¯å¦ç¬¦åˆè§’è‰²æ€§æ ¼å’Œå½“å‰çŠ¶æ€",
            "2. **åœºæ™¯è¦†ç›–**: æ˜¯å¦è¦†ç›–æ‰€æœ‰åœºæ™¯å¤§çº²è¦ç‚¹",
            "3. **é€»è¾‘åˆç†æ€§**: æƒ…èŠ‚å‘å±•æ˜¯å¦åˆç†ï¼Œæœ‰æ— æ˜æ˜¾æ¼æ´",
            "4. **ä¼ç¬”å¤„ç†**: å¦‚æœ‰ä¼ç¬”ï¼Œæ˜¯å¦è‡ªç„¶èå…¥æˆ–æ¨è¿›",
            "5. **æ–‡ç¬”è´¨é‡**: æ˜¯å¦å±•ç¤ºä¸å‘ŠçŸ¥ï¼Œç»†èŠ‚æ˜¯å¦ä¸°å¯Œ",
            "",
            "ã€å›ç­”æ ¼å¼ã€‘",
            "- å¦‚æœé€šè¿‡: 'é€šè¿‡ - äº®ç‚¹: [å…·ä½“è¯´æ˜]'",
            "- å¦‚æœéœ€ä¿®æ”¹: 'éœ€ä¿®æ”¹ - é—®é¢˜: [å…·ä½“é—®é¢˜]'",
            "",
            "è¯·ç»™å‡ºä¸“ä¸šè¯„å®¡ã€‚"
        ])

    prompt = '\n'.join(prompt_parts)

    max_attempts = 2
    for attempt in range(max_attempts):
        try:
            llm = ChatAnthropic(
                model="claude-sonnet-4-5-20250929",
                temperature=0.3,  # ç¨é«˜æ¸©åº¦,æ›´çµæ´»çš„è¯„å®¡
                anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
                anthropic_api_url=os.getenv("ANTHROPIC_BASE_URL"),
                timeout=90.0,  # å¢åŠ åˆ°90ç§’,é¿å…è¯„å®¡è¶…æ—¶
                max_retries=0
            )

            response = llm.invoke([HumanMessage(content=prompt)])
            feedback = response.content.strip()

            print(f"  âœ… è¯„å®¡å®Œæˆ")

            # æ˜¾ç¤ºè¯„å®¡ç»“æœæ‘˜è¦ (ä¼˜å…ˆæ£€æŸ¥"éœ€ä¿®æ”¹"ï¼Œå› ä¸ºè¿™æ˜¯æ›´é‡è¦çš„çŠ¶æ€)
            if "éœ€ä¿®æ”¹" in feedback or "ä¸åˆæ ¼" in feedback:
                print(f"     çŠ¶æ€: âš ï¸  éœ€æ”¹è¿›")
            elif "é€šè¿‡" in feedback or "åˆæ ¼" in feedback:
                print(f"     çŠ¶æ€: âœ… é€šè¿‡")
            else:
                print(f"     çŠ¶æ€: â“ æœªçŸ¥")

            return {"feedback": feedback}

        except Exception as e:
            if attempt < max_attempts - 1:
                wait = (attempt + 1) * 4
                print(f"  â³ è¯„å®¡è¶…æ—¶,é‡è¯• ({attempt+2}/{max_attempts})...")
                time.sleep(wait)
            else:
                print(f"  âš ï¸  è¯„å®¡è¶…æ—¶,ä½¿ç”¨å¿«é€Ÿæ£€æŸ¥")
                # å¿«é€Ÿæœ¬åœ°æ£€æŸ¥
                local_feedback = quick_local_check(draft, world_bible, is_fanqie)
                return {"feedback": local_feedback}


def quick_local_check(draft, world_bible, is_fanqie=False):
    """æ— AIçš„å¿«é€Ÿæœ¬åœ°è´¨é‡æ£€æŸ¥"""
    issues = []

    # æ£€æŸ¥é•¿åº¦
    if len(draft) < 800:
        issues.append("å†…å®¹è¿‡çŸ­")

    if is_fanqie:
        # ç•ªèŒ„å°è¯´ä¸“ç”¨æ£€æŸ¥
        # æ£€æŸ¥èŠ‚å¥ï¼ˆé¿å…è¿‡é•¿æ®µè½ï¼‰
        paragraphs = draft.split('\n\n')
        long_paragraphs = [p for p in paragraphs if len(p) > 500]
        if len(long_paragraphs) > 2:
            issues.append("æ®µè½è¿‡é•¿ï¼ŒèŠ‚å¥æ‹–æ²“")

        # æ£€æŸ¥å¯¹è¯ï¼ˆç•ªèŒ„å°è¯´éœ€è¦æ›´å¤šå¯¹è¯ï¼‰
        dialogue_count = draft.count('ã€Œ') + draft.count('"') + draft.count('"')
        if dialogue_count < 4 and len(draft) > 1000:
            issues.append("å¯¹è¯å¤ªå°‘ï¼Œéœ€è¦æ›´å¤šç›´æ¥å¯¹è¯")

        # æ£€æŸ¥çˆ½ç‚¹å…³é”®è¯
        shuangdian_keywords = ['ç¢¾å‹', 'æ‰“è„¸', 'éœ‡æƒŠ', 'ä¸æ•¢ç›¸ä¿¡', 'æ€ä¹ˆå¯èƒ½', 'æ”¶è·', 'çªç ´']
        has_shuangdian = any(kw in draft for kw in shuangdian_keywords)
        if not has_shuangdian and len(draft) > 1500:
            issues.append("ç¼ºå°‘çˆ½ç‚¹å…ƒç´ ï¼ˆæ‰“è„¸/ç¢¾å‹/æ”¶è·ï¼‰")

    else:
        # ä¼ ç»Ÿæ–‡å­¦æ£€æŸ¥
        # æ£€æŸ¥è¿‡åº¦ä½¿ç”¨çš„è¯æ±‡
        overused_words = ['çªç„¶', 'åŸæ¥', 'ç«Ÿç„¶', 'åªè§', 'åªå¬']
        for word in overused_words:
            count = draft.count(word)
            if count > 4:
                issues.append(f"'{word}'ä½¿ç”¨è¿‡å¤š({count}æ¬¡)")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹è¯
        has_dialogue = ('ã€Œ' in draft or '"' in draft or '"' in draft or
                        'ã€Œ' in draft or 'ã€' in draft)
        if not has_dialogue and len(draft) > 1000:
            issues.append("ç¼ºå°‘å¯¹è¯")

    # æ£€æŸ¥è§’è‰²åç§°æ˜¯å¦å‡ºç°
    characters = world_bible.get('characters', {})
    char_mentioned = False
    for char_name in characters.keys():
        if char_name in draft:
            char_mentioned = True
            break

    if not char_mentioned and characters:
        issues.append("æœªæåŠä¸»è¦è§’è‰²")

    if issues:
        return f"éœ€æ”¹è¿›: {'; '.join(issues[:3])}"
    else:
        style_type = "ç•ªèŒ„å°è¯´æ ‡å‡†" if is_fanqie else "ä¼ ç»Ÿæ ‡å‡†"
        return f"é€šè¿‡(æœ¬åœ°æ£€æŸ¥/{style_type}): å†…å®¹é•¿åº¦é€‚ä¸­,æ ¼å¼åˆç†,åŒ…å«å¿…è¦å…ƒç´ "


def extract_character_context(characters):
    """æå–è§’è‰²ä¸Šä¸‹æ–‡ï¼ˆå®Œæ•´ç‰ˆåŠŸèƒ½ï¼‰"""
    lines = []
    for name, char_data in list(characters.items())[:3]:
        notes = char_data.get("notes", [])
        if notes:
            latest = notes[-1] if isinstance(notes[-1], str) else str(notes[-1])
            lines.append(f"- {name}: {latest[:80]}")
    return "\n".join(lines) if lines else ""
